# VideoMind: An Omni-Modal Video Dataset with Intent Grounding for Deep-Cognitive Video Understanding \[[Paper]()\]

# :fire: News
- We release the full version of the video annotation for VideoMind ([OpenDataLab](https://opendatalab.com/Dixin/VideoMind) | [HuggingFace]()). 
  
# :book: Introduction

### What is VideoMind?
VideoMind is a large-scale video-centric multimodal dataset that can be used to learn powerful and transferable text-video representations for video understanding tasks such as video question answering and video retrieval. 

![b469e00b43d46a6b3f89899483abcf6](https://github.com/cdx-cindy/VideoMind/blob/main/image/cfdd6d79178d49e0e217ccc7ec7fa2e3.jpg)

### Data statistics
![b469e00b43d46a6b3f89899483abcf6](https://github.com/cdx-cindy/VideoMind/blob/main/image/cfdd6d79178d49e0e217ccc7ec7fa2e3.jpg)

### Model Performance

# :arrow_down: Data & Model Zoo

### Download
You can download our video annotation from \[[OpenDataLab](https://opendatalab.com/Dixin/VideoMind) \| [HuggingFace](https://opendatalab.com/Dixin/VideoMind) \]

You can download the raw videos and original annotation from \[[InternVid](https://opendatalab.com/shepshep/InternVid) \].

## Citation
If you find this work useful for your research, please consider citing VideoMind. Your acknowledgement would greatly help us in continuing to contribute resources to the research community. ðŸ˜Š
```
@inproceedings{,
  title={VideoMind: An Omni-Modal Video Dataset with Intent Grounding for Deep-Cognitive Video Understanding},
  author={},
  booktitle={},
  year={}
}
```

